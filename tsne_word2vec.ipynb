{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from time import time\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "print(f\"Gensim version: {gensim.__version__}\") \n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/tdelatte/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/tdelatte/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/tdelatte/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doc(doc):\n",
    "    # Lowercase\n",
    "    doc = doc.lower()\n",
    "    # Remove numbers\n",
    "    doc = re.sub(r\"[0-9]+\", \"\", doc)\n",
    "    # Split in tokens\n",
    "    tokens = doc.split()\n",
    "    # Remove punctuation\n",
    "    tokens = [w.translate(str.maketrans('', '', string.punctuation)) for w in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def read_files(path):\n",
    "    documents = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "    clean = []\n",
    "    tokenize  = lambda x: gensim.utils.simple_preprocess(x)\n",
    "    for d in documents:\n",
    "        with open(f\"{path}/{d}\", encoding='utf-8') as f:\n",
    "            doc = f.read()\n",
    "            doc = clean_doc(doc)\n",
    "            clean.append(tokenize(doc))\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory with raw files\n",
    "TEXT_DIR  = \"/home/tdelatte/projects/notebooks/data/word_embeddings/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 7\n"
     ]
    }
   ],
   "source": [
    "# Load and clean data\n",
    "docs = read_files(TEXT_DIR)\n",
    "\n",
    "print('Number of documents: %i' % len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-06-17 15:53:20--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.137.62\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.137.62|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1647046227 (1.5G) [application/x-gzip]\n",
      "Saving to: ‘projects/notebooks/data/GoogleNews-vectors-negative300.bin.gz’\n",
      "\n",
      "GoogleNews-vectors- 100%[===================>]   1.53G   671KB/s    in 36m 15s \n",
      "\n",
      "2020-06-17 16:29:36 (739 KB/s) - ‘projects/notebooks/data/GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get Google word embeddings\n",
    "!wget -P projects/notebooks/data/ -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-17 16:35:04,055 : INFO : loading projection weights from /home/tdelatte/projects/notebooks/data/word_embeddings/GoogleNews-vectors-negative300.bin.gz\n",
      "2020-06-17 16:56:02,161 : INFO : loaded (3000000, 300) matrix from /home/tdelatte/projects/notebooks/data/word_embeddings/GoogleNews-vectors-negative300.bin.gz\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_FILE = '/home/tdelatte/projects/notebooks/data/word_embeddings/GoogleNews-vectors-negative300.bin.gz' \n",
    "google_word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-17 16:56:36,964 : INFO : collecting all words and their counts\n",
      "2020-06-17 16:56:36,966 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-06-17 16:56:37,486 : INFO : collected 22425 word types from a corpus of 1103523 raw words and 7 sentences\n",
      "2020-06-17 16:56:37,489 : INFO : Loading a fresh vocabulary\n",
      "2020-06-17 16:56:37,542 : INFO : effective_min_count=20 retains 3747 unique words (16% of original 22425, drops 18678)\n",
      "2020-06-17 16:56:37,543 : INFO : effective_min_count=20 leaves 1030346 word corpus (93% of original 1103523, drops 73177)\n",
      "2020-06-17 16:56:37,611 : INFO : deleting the raw counts dictionary of 22425 items\n",
      "2020-06-17 16:56:37,613 : INFO : sample=0.001 downsamples 53 most-common words\n",
      "2020-06-17 16:56:37,615 : INFO : downsampling leaves estimated 754522 word corpus (73.2% of prior 1030346)\n",
      "2020-06-17 16:56:38,477 : INFO : estimated required memory for 3747 words and 300 dimensions: 10866300 bytes\n",
      "2020-06-17 16:56:38,478 : INFO : resetting layer weights\n",
      "2020-06-17 16:56:40,733 : INFO : loading projection weights from /home/tdelatte/projects/notebooks/data/word_embeddings/GoogleNews-vectors-negative300.bin.gz\n",
      "2020-06-17 17:06:12,784 : INFO : merged 3552 vectors into (3747, 300) matrix from /home/tdelatte/projects/notebooks/data/word_embeddings/GoogleNews-vectors-negative300.bin.gz\n",
      "2020-06-17 17:06:14,137 : INFO : training model with -1 workers on 3747 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-06-17 17:06:16,219 : INFO : EPOCH - 1 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2020-06-17 17:06:16,233 : WARNING : EPOCH - 1 : supplied example count (0) did not equal expected count (7)\n",
      "2020-06-17 17:06:16,252 : INFO : EPOCH - 2 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2020-06-17 17:06:16,894 : WARNING : EPOCH - 2 : supplied example count (0) did not equal expected count (7)\n",
      "2020-06-17 17:06:16,898 : INFO : EPOCH - 3 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2020-06-17 17:06:16,913 : WARNING : EPOCH - 3 : supplied example count (0) did not equal expected count (7)\n",
      "2020-06-17 17:06:16,917 : INFO : EPOCH - 4 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2020-06-17 17:06:16,931 : WARNING : EPOCH - 4 : supplied example count (0) did not equal expected count (7)\n",
      "2020-06-17 17:06:16,947 : INFO : EPOCH - 5 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2020-06-17 17:06:16,948 : WARNING : EPOCH - 5 : supplied example count (0) did not equal expected count (7)\n",
      "2020-06-17 17:06:16,951 : INFO : training on a 0 raw words (0 effective words) took 2.8s, 0 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 18s, sys: 0 ns, total: 7min 18s\n",
      "Wall time: 9min 42s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "google_model = Word2Vec(size = 300, window=5, min_count = 20, workers = -1)\n",
    "google_model.build_vocab(docs)\n",
    "\n",
    "google_model.intersect_word2vec_format(EMBEDDING_FILE, lockf=1.0, binary=True)\n",
    "\n",
    "google_model.train(docs, total_examples=google_model.corpus_count, epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-17 17:07:17,167 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('neville', 0.5812051296234131),\n",
       " ('mr', 0.5559774041175842),\n",
       " ('dennis', 0.5308670997619629),\n",
       " ('james', 0.5300297737121582),\n",
       " ('davies', 0.5283474922180176),\n",
       " ('arry', 0.520870566368103),\n",
       " ('johnson', 0.5157880783081055),\n",
       " ('roberts', 0.4989515244960785),\n",
       " ('dont', 0.49838465452194214),\n",
       " ('charlie', 0.49837130308151245)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_model.wv.most_similar(positive=[\"harry\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spinnet', 0.17876383662223816),\n",
       " ('krum', 0.1773030161857605),\n",
       " ('umbridge', 0.17520183324813843),\n",
       " ('frog', 0.16392986476421356),\n",
       " ('lupin', 0.15984302759170532),\n",
       " ('arithmancy', 0.15299880504608154),\n",
       " ('figure', 0.15231657028198242),\n",
       " ('mum', 0.15043088793754578),\n",
       " ('newt', 0.1451343595981598),\n",
       " ('lily', 0.14472904801368713)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_model.wv.most_similar(positive=[\"voldemort\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dementors', 0.6436929702758789),\n",
       " ('hippogriff', 0.5677863955497742),\n",
       " ('dementor', 0.5650946497917175),\n",
       " ('creature', 0.5508249998092651),\n",
       " ('gargoyle', 0.5491913557052612),\n",
       " ('spider', 0.5463742017745972),\n",
       " ('serpent', 0.5414613485336304),\n",
       " ('goblin', 0.5300490260124207),\n",
       " ('horcrux', 0.5296100378036499),\n",
       " ('spiders', 0.5049328804016113)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_model.wv.most_similar(positive=[\"basilisk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('wands', 0.7150397300720215),\n",
       " ('broom', 0.531521201133728),\n",
       " ('broomstick', 0.46065953373908997),\n",
       " ('dementors', 0.4059273600578308),\n",
       " ('hippogriff', 0.38803809881210327),\n",
       " ('basilisk', 0.3835200071334839),\n",
       " ('enchantments', 0.38186854124069214),\n",
       " ('sword', 0.38140869140625),\n",
       " ('dementor', 0.3698751926422119),\n",
       " ('gently', 0.3696576654911041)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_model.wv.most_similar(positive=[\"wand\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'harry' and 'ron' is 0.47980862855911255\n"
     ]
    }
   ],
   "source": [
    "sim = google_model.wv.similarity('harry', 'ron')\n",
    "print(\"Similarity between 'harry' and 'ron' is {}\".format(sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word that does not belong in the given list is : library\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tdelatte/miniconda3/envs/tf/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/tdelatte/miniconda3/envs/tf/lib/python3.7/site-packages/gensim/models/keyedvectors.py:877: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    }
   ],
   "source": [
    "odd = google_model.doesnt_match(['harry', 'ron', 'hermione', 'library'])\n",
    "print(f\"word that does not belong in the given list is : {odd}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('prince', 0.800979733467102), ('knight', 0.6788281202316284), ('mistress', 0.6786445379257202), ('witch', 0.6756536364555359), ('doge', 0.6740080118179321)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tdelatte/miniconda3/envs/tf/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar_cosmul` (Method will be removed in 4.0.0, use self.wv.most_similar_cosmul() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "print(google_model.most_similar_cosmul(positive=[\"woman\", \"king\"], negative=[\"man\"], topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('deluminator', 0.6585240960121155), ('risk', 0.6410618424415588), ('directly', 0.6293377876281738), ('lockhart', 0.6256893873214722), ('ogden', 0.6213969588279724)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tdelatte/miniconda3/envs/tf/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar_cosmul` (Method will be removed in 4.0.0, use self.wv.most_similar_cosmul() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "print(google_model.most_similar_cosmul(positive=[\"gryffindor\", \"malfoy\"], negative=[\"potter\"], topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-SNE Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspired by code here: https://www.kaggle.com/jeffd23/visualizing-word-vectors-with-t-sne\n",
    "def tsne_plot(model):\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "\n",
    "    for word in model.wv.vocab:\n",
    "        tokens.append(model[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(n_components=2, init=\"pca\", n_iter=1000, random_state=32)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(16, 16)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "tsne_plot(google_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
